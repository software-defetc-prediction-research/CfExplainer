# introduce
  CfExplainer, a rule-based method for explainable defect prediction, the explanations of the proposed approach can enhance the model credibility and help guide developers in fixing defects and reducing the risk of introducing them.
# DataSet
 we selected Openstack and Qt from the open source software projects provided by McIntosh and Kamei
# global model
Build the global model (jit_model.py)
# CfExplainer Method
  1. Generating synthetic instances：
     1.1 Similar Instance Selection: Feature normalisation, Calculate the instance similarity score, Select top-N similar instances
     1.2 Counterfactual generation of synthetic instances
  2. Synthetic Instance Prediction
  3. Building Local Model
     3.1 Rule Generation(Apriori)
     3.2 Rule Ranking
     3.3 Rule Pruning
     3.4 Multi-Rule Prediction
  4. Rules Explanation
# Research Questions
  1. RQ1: How similar are the synthetic instances generated by CfExplainer to the instances to be explained?
  2. RQ2: Does our CfExplainer have higher local model fitness than other baseline methods?
  3. RQ3: How reliable is the CfExplainer explanation?
# experiments
  1. local model：The local models of CfExplainer, PyExplainer and LIME are weighted class association rules（rule_minning.py）, RuleFit(rule_fit.py) , K-Lasso(K-lasso.py) respectively
  2. weighted class association rules：
	  2.1 Rule Generation(Apriori)：def apriori_generate_rule（）；
	  2.2 Rule Ranking；def rank_rule(）；
	  2.3 Rule Pruning：def remove_conflict_rule()，def remove_redundant_rule(）
	  2.4 Multi-Rule Prediction：def rules_predict()
	  2.5 generate instance method：random_perturbatio.py, crossover_interpolation.py, counterfactual_ generation.py
	  

